<!doctype html>
<html>
<head>
<meta charset="gb2312">
<title>ly's blog</title>

<link href="css/styles.css" rel="stylesheet">
<!--[if lt IE 9]>
<script src="js/modernizr.js"></script>
<![endif]-->
</head>
<body>
<article>
 
  
   
   
  <div class="rightbox box">
    <h2>ly's  BLOG</h2>
    <h3 class="title"><a href="/">开发者需要知道的IOS11新特性</a></h3>
    <ul class="text">
      <p>年纪大了过了能熬夜看 WWDC 的时代了，但是还是在小小宝的哭闹和妈妈大人换尿布的催促中起了个大早。于是算趁着“热乎”把 WWDC 2017 的 Keynote 看完了。和往年差不多，虽然 WWDC 是一个开发者会议，但是 Keynote 并不是专门针对我们开发者的，它还承担了公司状况说明，新品发布等功能。作为技术人员，可能接下来的 session 会更有意义。要用一句话来评价今年 Keynote 所展现出来的内容的话，就是小步革新。大的技术方面可以说只有 ARKit 可堪研究，但是我们还是看到了类似跨 app 拖拽，新的 Files 应用这样进一步突破 iOS 原有桎梏的更新 (iMessage 转账什么的就不提了，我大天朝威武，移动支付领域领先世界至少三年)。iOS 11，特别是配合新的硬件，相信会给用户带来不错的体验。

作为 iOS 开发者，和往年一样，我整理了一下在可能需要关注的地方</p>
    <h4>新增框架</h4>
    <p>新加入 SDK 的大的框架有两个，分别是负责简化和集成机器学习的 Core ML 和用来创建增强现实 (AR) 应用的 ARKit。</p>
    <h4>Core ML</h4>
    <p>自从 AlphaGo 出现以来，深度学习毫无疑问成了行业热点。而 Google 也在去年就转变 Mobile-first 到 AI-first 的战略。可以说一线的互联网企业几乎都在押宝 AI，目前看来机器学习，特别是深度学习是最有希望的一条道路。

如果你不是很熟悉机器学习的话，我想我可以在这里“僭越”地做一些简介。你可以先把机器学习的模型看作一个黑盒函数，你给定一些输入 (可能是一段文字，或者一张图片)，这个函数会给出特定的输出 (比如这段文字中的人名地名，或者图片中出现的商店名牌等)。一开始这个模型可能非常粗糙，完全不能给出正确的结果，但是你可以使用大量已有的数据和正确的结果，来对模型进行训练，甚至改进。在所使用的模型足够优化，以及训练量足够大的情况下，这个黑盒模型将不仅对训练数据有较高的准确率，也往往能对未知的实际输入给出正确的返回。这样的模型就是一个训练好的可以实际使用的模型。

对机器学习模型的训练是一项很重的工作，Core ML 所扮演的角色更多的是将已经训练好的模型转换为 iOS 可以理解的形式，并且将新的数据“喂给”模型，获取输出。抽象问题和创建模型虽然并不难，但是对模型的改进和训练可以说是值得研究一辈子的事情，这篇文章的读者可能也不太会对此感冒。好在 Apple 提供了一系列的工具用来将各类机器学习模型转换为 Core ML 可以理解的形式。籍此，你就可以轻松地在你的 iOS app 里使用前人训练出的模型。这在以前可能会需要你自己去寻找模型，然后写一些 C++ 的代码来跨平台调用，而且难以利用 iOS 设备的 GPU 性能和 Metal (除非你自己写一些 shader 来进行矩阵运算)。Core ML 将使用模型的门槛降低了很多。

Core ML 在背后驱动了 iOS 的视觉识别的 Vision 框架和 Foundation 中的语义分析相关 API。普通开发者可以从这些高层的 API 中直接获益，比如人脸图片或者文字识别等。这部分内容在以前版本的 SDK 中也存在，不过在 iOS 11 SDK 中它们被集中到了新的框架中，并将一些更具体和底层的控制开放出来。比如你可以使用 Vision 中的高层接口，但是同时指定底层所使用的模型。这给 iOS 的计算机视觉带来了新的可能。

Google 或者 Samsung 在 Android AI 上的努力，大多是在自带的应用中集成服务。相比起来，Apple 基于对自己生态和硬件的控制，将更多的选择权交给了第三方开发者。</p>

     <h4>ARKit</h4>
     <p>Keynote 上的 AR 的演示可以说是唯一的亮点了。iOS SDK 11 中 Apple 给开发者，特别是 AR 相关的开发者带来了一个很棒的礼物，那就是 ARKit。AR 可以说并非什么新技术，像是 Pokémon Go 这样的游戏也验证了 AR 在游戏上的潜力。不过除了 IP 和新鲜感之外，个人认为 Pokémon Go 并没有资格代表 AR 技术的潜力。现场的演示像我们展示了一种可能，粗略看来，ARKit 利用单镜头和陀螺仪，在对平面的识别和虚拟物体的稳定上做得相当出色。几乎可以肯定，那么不做最早，只做最好的 Apple 似乎在这一刻回到了舞台上

ARKit 极大降低了普通开发者玩 AR 的门槛，也是 Apple 现阶段用来抗衡 VR 的选项。可以畅想一下更多类似 Pokémon Go 的 AR 游戏 (结合实境的虚拟宠物什么的大概是最容易想到的) 能在 ARKit 和 SceneKit 的帮助下面世，甚至在 iPad Pro 现有技能上做像是 AR 电影这样能全方位展示的多媒体可能也不再是单纯的梦想。

而与之相应的，是一套并不很复杂的 API。涉及的 View 几乎是作为 SceneKit 的延伸，再加上在真实世界的定为也已经由系统帮助处理，开发者需要做的大抵就是将虚拟物体放在屏幕的合适位置，并让物体之间互动。而利用 Core ML 来对相机内的实际物体进行识别和交互，可以说也让各类特效的相机或者摄影 app 充满了想像空间。</p>
     
    <div class="textfoot"> <a href="/">阅读全文</a><a href="/">评论</a><a href="/">转载</a> </div>
    <div class="pages"><span>1</span><a href="/" hidefocus="">2</a><a href="/" hidefocus="">3</a><a href="/" class="next">下一页&gt;&gt;</a></div>
  </div>
  <div class="blank"></div>
  <div class="Copyright">
    
    <p>Design by Cxy</p>
  </div>
</article>
</body>
</html>